<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dipika Bogati</title>
</head>
<body>
    <h1>DIPIKA BOGATI</h1>
    <img src="images/profilePicture.jpg">
    <p>Experienced Data Engineer with more than 3+ years experience in SQL, Python and AWS services. Worked with
micro-services to setup data pipeline (including Ingestion, Transformation and Load) in AWS using Python,
Databases, AWS Lambda and AWS Batch. Effective communication both oral and written skills and demonstrated
the professional collaborative work for 3+ years in IT industry while working with US-based projects.</p>

    <h3>
        Experiences:
    </h3>
    <ol>

            <b>
                Graudate Teaching Assistant
            </b>, Bowling Green State University Bowling Green, OH, US January 2025 - Present
            <ul>
            <li>
                Developed the learning program in unity which helps student to interact with object while learning statics.
            </li>
            <li>
                Migrated the statics learning program into Meta Quest 3s.
            </li>
            <li>
                Integrated llama2 into the program to fetch some response instantly.
            </li>
            </ul>
            <b>
                Data Engineer
            </b>, Verisk Lalitpur, Nepal August 2021 – August 2024
            <ul>
            <li>Worked in Verisk’s central Data Hub team, the core group responsible for enterprise-wide ETL services, focusing
on building scalable data pipelines.</li>
            <li>Added micro-services in the ETL pipeline where data is extracted from databases (Oracle, Aurora, DynamoDB,
Snowflake, SQL Server, PostgreSQL), then transformed and loaded into Snowflake warehouse.</li>
            <li>Integrated several AWS services (s3 bucket, batch, lambda, SNS, SQS, RDS) to automate data pipeline.</li>
            <li>Implemented mliti-processing to handle big data (Terabytes) and make execution process faster. </li>
            <li>Wrote unit test cases in Python and increased the SONAR code coverage and decreased the vlinerabilities from
the Python code.</li>
            <li>Worked on version control system (git, bitbucket) to maintain code version.</li>
            <li>Migrated generic data validation service from Pentaho to python which validate the data distributes to mlitiple
teams.</li>
            <li>Interviewed and mentored for Data Engineer trainees.</li>
            </ul>
            <b>
                Full Stack Developer
            </b>, Curllabs Bhaktapur, Nepal August 2020 – October 2020
            <ul>
                <li>Designed and created database to store and manage information on popular routes across Nepal.</li>
                <li>Created web pages using HTML, CSS, Bootstrap and JavaScript.</li>
                <li>Implemented CRUD operations for admin to manipulate the routes.</li>
            </ul>
            </ol>

    <h3>
        Skills:
    </h3>
    <b>
        Data Engineering:
    </b> Python, SQL, Snowflake, Amazon Web Services (AWS) <br>
    <b>
        Programming Languages:
    </b> Python, Java, Java Script, C# <br>
    <b>
        Technologies:
    </b> Object-oriented Programming, Cloud Computing, Multi-processing, Batch Script <br>
    <b>
        Soft Skills:
    </b> Proactive, enthusiastic, Attention to Detail
    
    <h3>
        Education
    </h3>
    <b>Masters in Computer Science</b>, Bowling Green State University Expected: May 2026 <br>
    GPA: 4.0<br>
    <b>Bachelors in Science in Information Technology in Computing</b>, London Metropolitan University December 2020 <br>
    GPA: 3.86 <br>

    <h3>
        Certifications and Awards
    </h3>
    <ul>Verisk Way To Go</ul> January 2024
    <ul>AWS Certified Cloud Practitioner</ul>, March 2023

    <h3>
        Projects
    </h3>
    <ul>
        <li><b>Real-Time Log Processing Pipeline with Kafka, Spark Streaming and Docker: </b>
        Built a real-time log monitoring system leveraging Apache Kafka for ingestion, Spark Streaming for processing,
PostgreSQL for storage, and Streamlit for visualization. The project enables instant error detection and analytics
while being containerized with Docker for seamless deployment.
        </li>
    </ul>
    
</body>
</html>